{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nnp.random.seed(1212)\n\nimport keras\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras import optimizers","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf_train.head() # 784 features, 1 label\n\n","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   label  pixel0  pixel1    ...     pixel781  pixel782  pixel783\n0      1       0       0    ...            0         0         0\n1      0       0       0    ...            0         0         0\n2      1       0       0    ...            0         0         0\n3      4       0       0    ...            0         0         0\n4      0       0       0    ...            0         0         0\n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>pixel10</th>\n      <th>pixel11</th>\n      <th>pixel12</th>\n      <th>pixel13</th>\n      <th>pixel14</th>\n      <th>pixel15</th>\n      <th>pixel16</th>\n      <th>pixel17</th>\n      <th>pixel18</th>\n      <th>pixel19</th>\n      <th>pixel20</th>\n      <th>pixel21</th>\n      <th>pixel22</th>\n      <th>pixel23</th>\n      <th>pixel24</th>\n      <th>pixel25</th>\n      <th>pixel26</th>\n      <th>pixel27</th>\n      <th>pixel28</th>\n      <th>pixel29</th>\n      <th>pixel30</th>\n      <th>pixel31</th>\n      <th>pixel32</th>\n      <th>pixel33</th>\n      <th>pixel34</th>\n      <th>pixel35</th>\n      <th>pixel36</th>\n      <th>pixel37</th>\n      <th>pixel38</th>\n      <th>...</th>\n      <th>pixel744</th>\n      <th>pixel745</th>\n      <th>pixel746</th>\n      <th>pixel747</th>\n      <th>pixel748</th>\n      <th>pixel749</th>\n      <th>pixel750</th>\n      <th>pixel751</th>\n      <th>pixel752</th>\n      <th>pixel753</th>\n      <th>pixel754</th>\n      <th>pixel755</th>\n      <th>pixel756</th>\n      <th>pixel757</th>\n      <th>pixel758</th>\n      <th>pixel759</th>\n      <th>pixel760</th>\n      <th>pixel761</th>\n      <th>pixel762</th>\n      <th>pixel763</th>\n      <th>pixel764</th>\n      <th>pixel765</th>\n      <th>pixel766</th>\n      <th>pixel767</th>\n      <th>pixel768</th>\n      <th>pixel769</th>\n      <th>pixel770</th>\n      <th>pixel771</th>\n      <th>pixel772</th>\n      <th>pixel773</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = df_train.iloc[:, 1:785]\ndf_label = df_train.iloc[:, 0]\n\nX_test = df_test.iloc[:, 0:784]\n\nprint(X_test.shape)","execution_count":5,"outputs":[{"output_type":"stream","text":"(28000, 784)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_cv, y_train, y_cv = train_test_split(df_features, df_label, \n                                                test_size = 0.2,\n                                                random_state = 1212)\n\nX_train = X_train.as_matrix().reshape(33600, 784) #(33600, 784)\nX_cv = X_cv.as_matrix().reshape(8400, 784) #(8400, 784)\n\nX_test = X_test.as_matrix().reshape(28000, 784)","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n  import sys\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n  if __name__ == '__main__':\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((min(X_train[1]), max(X_train[1])))","execution_count":7,"outputs":[{"output_type":"stream","text":"(0, 255)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Normalization \nX_train = X_train.astype('float32'); X_cv= X_cv.astype('float32'); X_test = X_test.astype('float32')\nX_train /= 255; X_cv /= 255; X_test /= 255\n\n# Convert labels to One Hot Encoded\nnum_digits = 10\ny_train = keras.utils.to_categorical(y_train, num_digits)\ny_cv = keras.utils.to_categorical(y_cv, num_digits)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing 2 examples of labels after conversion\nprint(y_train[0]) # 2\nprint(y_train[3]) # 7","execution_count":9,"outputs":[{"output_type":"stream","text":"[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input Parameters\nn_input = 784 # number of features\nn_hidden_1 = 300\nn_hidden_2 = 100\nn_hidden_3 = 100\nn_hidden_4 = 200\nnum_digits = 10","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Inp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our model would have '6' layers - input layer, 4 hidden layer and 1 output layer\nmodel = Model(Inp, output)\nmodel.summary() # We have 297,910 parameters to estimate","execution_count":12,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 784)               0         \n_________________________________________________________________\nHidden_Layer_1 (Dense)       (None, 300)               235500    \n_________________________________________________________________\nHidden_Layer_2 (Dense)       (None, 100)               30100     \n_________________________________________________________________\nHidden_Layer_3 (Dense)       (None, 100)               10100     \n_________________________________________________________________\nHidden_Layer_4 (Dense)       (None, 200)               20200     \n_________________________________________________________________\nOutput_Layer (Dense)         (None, 10)                2010      \n=================================================================\nTotal params: 297,910\nTrainable params: 297,910\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Insert Hyperparameters\nlearning_rate = 0.1\ntraining_epochs = 20\nbatch_size = 100\nsgd = optimizers.SGD(lr=learning_rate)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We rely on the plain vanilla Stochastic Gradient Descent as our optimizing methodology\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model.fit(X_train, y_train,\n                     batch_size = batch_size,\n                     epochs = training_epochs,\n                     verbose = 2,\n                     validation_data=(X_cv, y_cv))","execution_count":16,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/20\n - 4s - loss: 1.8541 - acc: 0.4985 - val_loss: 1.0047 - val_acc: 0.7601\nEpoch 2/20\n - 2s - loss: 0.6482 - acc: 0.8293 - val_loss: 0.4641 - val_acc: 0.8720\nEpoch 3/20\n - 2s - loss: 0.4099 - acc: 0.8834 - val_loss: 0.3621 - val_acc: 0.8975\nEpoch 4/20\n - 2s - loss: 0.3377 - acc: 0.9027 - val_loss: 0.3123 - val_acc: 0.9104\nEpoch 5/20\n - 2s - loss: 0.2981 - acc: 0.9138 - val_loss: 0.2892 - val_acc: 0.9175\nEpoch 6/20\n - 2s - loss: 0.2684 - acc: 0.9227 - val_loss: 0.2652 - val_acc: 0.9243\nEpoch 7/20\n - 2s - loss: 0.2454 - acc: 0.9297 - val_loss: 0.2557 - val_acc: 0.9258\nEpoch 8/20\n - 2s - loss: 0.2273 - acc: 0.9351 - val_loss: 0.2321 - val_acc: 0.9336\nEpoch 9/20\n - 2s - loss: 0.2102 - acc: 0.9377 - val_loss: 0.2175 - val_acc: 0.9363\nEpoch 10/20\n - 2s - loss: 0.1952 - acc: 0.9440 - val_loss: 0.2053 - val_acc: 0.9394\nEpoch 11/20\n - 2s - loss: 0.1828 - acc: 0.9469 - val_loss: 0.1953 - val_acc: 0.9427\nEpoch 12/20\n - 2s - loss: 0.1708 - acc: 0.9504 - val_loss: 0.1850 - val_acc: 0.9446\nEpoch 13/20\n - 2s - loss: 0.1612 - acc: 0.9530 - val_loss: 0.1804 - val_acc: 0.9457\nEpoch 14/20\n - 2s - loss: 0.1515 - acc: 0.9560 - val_loss: 0.1762 - val_acc: 0.9471\nEpoch 15/20\n - 2s - loss: 0.1431 - acc: 0.9588 - val_loss: 0.1656 - val_acc: 0.9501\nEpoch 16/20\n - 2s - loss: 0.1353 - acc: 0.9605 - val_loss: 0.1594 - val_acc: 0.9530\nEpoch 17/20\n - 2s - loss: 0.1289 - acc: 0.9621 - val_loss: 0.1545 - val_acc: 0.9530\nEpoch 18/20\n - 2s - loss: 0.1217 - acc: 0.9646 - val_loss: 0.1478 - val_acc: 0.9560\nEpoch 19/20\n - 2s - loss: 0.1157 - acc: 0.9670 - val_loss: 0.1473 - val_acc: 0.9557\nEpoch 20/20\n - 2s - loss: 0.1100 - acc: 0.9683 - val_loss: 0.1409 - val_acc: 0.9582\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Inp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)\n\n# We rely on ADAM as our optimizing methodology\nadam = keras.optimizers.Adam(lr=learning_rate)\nmodel2 = Model(Inp, output)\n\nmodel2.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit(X_train, y_train,\n                      batch_size = batch_size,\n                      epochs = training_epochs,\n                      verbose = 2,\n                      validation_data=(X_cv, y_cv))","execution_count":18,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/20\n - 3s - loss: 0.3390 - acc: 0.8988 - val_loss: 0.1625 - val_acc: 0.9488\nEpoch 2/20\n - 2s - loss: 0.1253 - acc: 0.9613 - val_loss: 0.1205 - val_acc: 0.9620\nEpoch 3/20\n - 2s - loss: 0.0838 - acc: 0.9735 - val_loss: 0.1081 - val_acc: 0.9669\nEpoch 4/20\n - 2s - loss: 0.0615 - acc: 0.9795 - val_loss: 0.0937 - val_acc: 0.9724\nEpoch 5/20\n - 2s - loss: 0.0456 - acc: 0.9849 - val_loss: 0.1065 - val_acc: 0.9675\nEpoch 6/20\n - 2s - loss: 0.0408 - acc: 0.9865 - val_loss: 0.0849 - val_acc: 0.9775\nEpoch 7/20\n - 2s - loss: 0.0319 - acc: 0.9895 - val_loss: 0.1169 - val_acc: 0.9710\nEpoch 8/20\n - 2s - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0935 - val_acc: 0.9767\nEpoch 9/20\n - 2s - loss: 0.0225 - acc: 0.9926 - val_loss: 0.1010 - val_acc: 0.9783\nEpoch 10/20\n - 2s - loss: 0.0196 - acc: 0.9935 - val_loss: 0.1358 - val_acc: 0.9704\nEpoch 11/20\n - 2s - loss: 0.0197 - acc: 0.9934 - val_loss: 0.1063 - val_acc: 0.9760\nEpoch 12/20\n - 2s - loss: 0.0237 - acc: 0.9929 - val_loss: 0.0967 - val_acc: 0.9773\nEpoch 13/20\n - 2s - loss: 0.0211 - acc: 0.9926 - val_loss: 0.1052 - val_acc: 0.9752\nEpoch 14/20\n - 2s - loss: 0.0164 - acc: 0.9945 - val_loss: 0.1136 - val_acc: 0.9767\nEpoch 15/20\n - 2s - loss: 0.0145 - acc: 0.9946 - val_loss: 0.1342 - val_acc: 0.9729\nEpoch 16/20\n - 2s - loss: 0.0109 - acc: 0.9964 - val_loss: 0.1266 - val_acc: 0.9744\nEpoch 17/20\n - 2s - loss: 0.0134 - acc: 0.9956 - val_loss: 0.1187 - val_acc: 0.9755\nEpoch 18/20\n - 2s - loss: 0.0107 - acc: 0.9966 - val_loss: 0.1101 - val_acc: 0.9781\nEpoch 19/20\n - 2s - loss: 0.0191 - acc: 0.9944 - val_loss: 0.1124 - val_acc: 0.9755\nEpoch 20/20\n - 2s - loss: 0.0100 - acc: 0.9969 - val_loss: 0.1226 - val_acc: 0.9757\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Inp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)\n\nlearning_rate = 0.01\nadam = keras.optimizers.Adam(lr=learning_rate)\nmodel2a = Model(Inp, output)\n\nmodel2a.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2a = model2a.fit(X_train, y_train,\n                        batch_size = batch_size,\n                        epochs = training_epochs,\n                        verbose = 2,\n                        validation_data=(X_cv, y_cv))","execution_count":20,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/20\n - 3s - loss: 0.3384 - acc: 0.8982 - val_loss: 0.1974 - val_acc: 0.9410\nEpoch 2/20\n - 2s - loss: 0.1243 - acc: 0.9610 - val_loss: 0.1328 - val_acc: 0.9592\nEpoch 3/20\n - 2s - loss: 0.0829 - acc: 0.9743 - val_loss: 0.1035 - val_acc: 0.9693\nEpoch 4/20\n - 2s - loss: 0.0618 - acc: 0.9802 - val_loss: 0.0921 - val_acc: 0.9733\nEpoch 5/20\n - 2s - loss: 0.0456 - acc: 0.9861 - val_loss: 0.1118 - val_acc: 0.9663\nEpoch 6/20\n - 2s - loss: 0.0398 - acc: 0.9870 - val_loss: 0.0935 - val_acc: 0.9754\nEpoch 7/20\n - 2s - loss: 0.0299 - acc: 0.9903 - val_loss: 0.1066 - val_acc: 0.9718\nEpoch 8/20\n - 2s - loss: 0.0231 - acc: 0.9927 - val_loss: 0.1098 - val_acc: 0.9737\nEpoch 9/20\n - 2s - loss: 0.0237 - acc: 0.9924 - val_loss: 0.1089 - val_acc: 0.9726\nEpoch 10/20\n - 2s - loss: 0.0230 - acc: 0.9923 - val_loss: 0.1032 - val_acc: 0.9752\nEpoch 11/20\n - 2s - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0957 - val_acc: 0.9781\nEpoch 12/20\n - 2s - loss: 0.0163 - acc: 0.9948 - val_loss: 0.1161 - val_acc: 0.9757\nEpoch 13/20\n - 2s - loss: 0.0187 - acc: 0.9943 - val_loss: 0.1048 - val_acc: 0.9754\nEpoch 14/20\n - 2s - loss: 0.0162 - acc: 0.9951 - val_loss: 0.1067 - val_acc: 0.9757\nEpoch 15/20\n - 2s - loss: 0.0131 - acc: 0.9961 - val_loss: 0.1126 - val_acc: 0.9748\nEpoch 16/20\n - 2s - loss: 0.0161 - acc: 0.9949 - val_loss: 0.1104 - val_acc: 0.9755\nEpoch 17/20\n - 2s - loss: 0.0106 - acc: 0.9963 - val_loss: 0.1158 - val_acc: 0.9761\nEpoch 18/20\n - 2s - loss: 0.0118 - acc: 0.9961 - val_loss: 0.1083 - val_acc: 0.9780\nEpoch 19/20\n - 2s - loss: 0.0147 - acc: 0.9960 - val_loss: 0.1197 - val_acc: 0.9761\nEpoch 20/20\n - 2s - loss: 0.0098 - acc: 0.9972 - val_loss: 0.1281 - val_acc: 0.9756\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Inp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)\n\nlearning_rate = 0.5\nadam = keras.optimizers.Adam(lr=learning_rate)\nmodel2b = Model(Inp, output)\n\nmodel2b.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2b = model2b.fit(X_train, y_train,\n                        batch_size = batch_size,\n                        epochs = training_epochs,\n                            validation_data=(X_cv, y_cv))","execution_count":22,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/20\n33600/33600 [==============================] - 3s 88us/step - loss: 0.3335 - acc: 0.9026 - val_loss: 0.1468 - val_acc: 0.9577\nEpoch 2/20\n33600/33600 [==============================] - 3s 78us/step - loss: 0.1240 - acc: 0.9625 - val_loss: 0.1316 - val_acc: 0.9590\nEpoch 3/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0799 - acc: 0.9747 - val_loss: 0.0995 - val_acc: 0.9699\nEpoch 4/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0560 - acc: 0.9824 - val_loss: 0.0991 - val_acc: 0.9702\nEpoch 5/20\n33600/33600 [==============================] - 2s 68us/step - loss: 0.0502 - acc: 0.9840 - val_loss: 0.0835 - val_acc: 0.9760\nEpoch 6/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0359 - acc: 0.9891 - val_loss: 0.0946 - val_acc: 0.9742\nEpoch 7/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0260 - acc: 0.9916 - val_loss: 0.1196 - val_acc: 0.9692\nEpoch 8/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0304 - acc: 0.9903 - val_loss: 0.0877 - val_acc: 0.9757\nEpoch 9/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0201 - acc: 0.9933 - val_loss: 0.0922 - val_acc: 0.9752\nEpoch 10/20\n33600/33600 [==============================] - 2s 68us/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.1007 - val_acc: 0.9764\nEpoch 11/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0161 - acc: 0.9947 - val_loss: 0.1347 - val_acc: 0.9690\nEpoch 12/20\n33600/33600 [==============================] - 2s 68us/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0876 - val_acc: 0.9782\nEpoch 13/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0185 - acc: 0.9942 - val_loss: 0.1089 - val_acc: 0.9735\nEpoch 14/20\n33600/33600 [==============================] - 2s 66us/step - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0972 - val_acc: 0.9786\nEpoch 15/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.1136 - val_acc: 0.9761\nEpoch 16/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0169 - acc: 0.9945 - val_loss: 0.1065 - val_acc: 0.9773\nEpoch 17/20\n33600/33600 [==============================] - 2s 68us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.1190 - val_acc: 0.9739\nEpoch 18/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.1058 - val_acc: 0.9779\nEpoch 19/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.1079 - val_acc: 0.9767\nEpoch 20/20\n33600/33600 [==============================] - 2s 67us/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.1326 - val_acc: 0.9718\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input Parameters\nn_input = 784 # number of features\nn_hidden_1 = 300\nn_hidden_2 = 100\nn_hidden_3 = 100\nn_hidden_4 = 100\nn_hidden_5 = 200\nnum_digits = 10","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Inp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\nx = Dense(n_hidden_5, activation='relu', name = \"Hidden_Layer_5\")(x)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our model would have '7' layers - input layer, 5 hidden layer and 1 output layer\nmodel3 = Model(Inp, output)\nmodel3.summary() # We have 308,010 parameters to estimate","execution_count":25,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         (None, 784)               0         \n_________________________________________________________________\nHidden_Layer_1 (Dense)       (None, 300)               235500    \n_________________________________________________________________\nHidden_Layer_2 (Dense)       (None, 100)               30100     \n_________________________________________________________________\nHidden_Layer_3 (Dense)       (None, 100)               10100     \n_________________________________________________________________\nHidden_Layer_4 (Dense)       (None, 100)               10100     \n_________________________________________________________________\nHidden_Layer_5 (Dense)       (None, 200)               20200     \n_________________________________________________________________\nOutput_Layer (Dense)         (None, 10)                2010      \n=================================================================\nTotal params: 308,010\nTrainable params: 308,010\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We rely on 'Adam' as our optimizing methodology\nadam = keras.optimizers.Adam(lr=0.01)\n\nmodel3.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history3 = model3.fit(X_train, y_train,\n                      batch_size = batch_size,\n                      epochs = training_epochs,\n                      validation_data=(X_cv, y_cv))","execution_count":27,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/20\n33600/33600 [==============================] - 3s 95us/step - loss: 0.3594 - acc: 0.8936 - val_loss: 0.1994 - val_acc: 0.9415\nEpoch 2/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.1246 - acc: 0.9619 - val_loss: 0.1224 - val_acc: 0.9635\nEpoch 3/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0834 - acc: 0.9737 - val_loss: 0.1134 - val_acc: 0.9662\nEpoch 4/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0607 - acc: 0.9807 - val_loss: 0.1083 - val_acc: 0.9694\nEpoch 5/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0494 - acc: 0.9846 - val_loss: 0.1186 - val_acc: 0.9671\nEpoch 6/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0379 - acc: 0.9877 - val_loss: 0.0935 - val_acc: 0.9760\nEpoch 7/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0361 - acc: 0.9878 - val_loss: 0.1099 - val_acc: 0.9706\nEpoch 8/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0288 - acc: 0.9903 - val_loss: 0.1165 - val_acc: 0.9733\nEpoch 9/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0255 - acc: 0.9922 - val_loss: 0.1043 - val_acc: 0.9736\nEpoch 10/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0234 - acc: 0.9928 - val_loss: 0.1061 - val_acc: 0.9740\nEpoch 11/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0211 - acc: 0.9933 - val_loss: 0.1087 - val_acc: 0.9738\nEpoch 12/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.1258 - val_acc: 0.9698\nEpoch 13/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0219 - acc: 0.9928 - val_loss: 0.1110 - val_acc: 0.9744\nEpoch 14/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0140 - acc: 0.9957 - val_loss: 0.1021 - val_acc: 0.9777\nEpoch 15/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0149 - acc: 0.9953 - val_loss: 0.1155 - val_acc: 0.9751\nEpoch 16/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0123 - acc: 0.9960 - val_loss: 0.1111 - val_acc: 0.9779\nEpoch 17/20\n33600/33600 [==============================] - 2s 68us/step - loss: 0.0159 - acc: 0.9949 - val_loss: 0.1094 - val_acc: 0.9780\nEpoch 18/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0165 - acc: 0.9950 - val_loss: 0.1250 - val_acc: 0.9744\nEpoch 19/20\n33600/33600 [==============================] - 2s 69us/step - loss: 0.0159 - acc: 0.9954 - val_loss: 0.1096 - val_acc: 0.9764\nEpoch 20/20\n33600/33600 [==============================] - 2s 70us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.1340 - val_acc: 0.9729\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input Parameters\nn_input = 784 # number of features\nn_hidden_1 = 300\nn_hidden_2 = 100\nn_hidden_3 = 100\nn_hidden_4 = 200\nnum_digits = 10","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Inp = Input(shape=(784,))\nx = Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\")(Inp)\nx = Dropout(0.3)(x)\nx = Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\")(x)\nx = Dropout(0.3)(x)\nx = Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\")(x)\nx = Dropout(0.3)(x)\nx = Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\")(x)\noutput = Dense(num_digits, activation='softmax', name = \"Output_Layer\")(x)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our model would have '6' layers - input layer, 4 hidden layer and 1 output layer\nmodel4 = Model(Inp, output)\nmodel4.summary() # We have 297,910 parameters to estimate","execution_count":30,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         (None, 784)               0         \n_________________________________________________________________\nHidden_Layer_1 (Dense)       (None, 300)               235500    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 300)               0         \n_________________________________________________________________\nHidden_Layer_2 (Dense)       (None, 100)               30100     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 100)               0         \n_________________________________________________________________\nHidden_Layer_3 (Dense)       (None, 100)               10100     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 100)               0         \n_________________________________________________________________\nHidden_Layer_4 (Dense)       (None, 200)               20200     \n_________________________________________________________________\nOutput_Layer (Dense)         (None, 10)                2010      \n=================================================================\nTotal params: 297,910\nTrainable params: 297,910\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model4.fit(X_train, y_train,\n                    batch_size = batch_size,\n                    epochs = training_epochs,\n                    validation_data=(X_cv, y_cv))","execution_count":32,"outputs":[{"output_type":"stream","text":"Train on 33600 samples, validate on 8400 samples\nEpoch 1/20\n33600/33600 [==============================] - 3s 97us/step - loss: 0.5818 - acc: 0.8125 - val_loss: 0.1879 - val_acc: 0.9415\nEpoch 2/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.2286 - acc: 0.9330 - val_loss: 0.1385 - val_acc: 0.9612\nEpoch 3/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.1759 - acc: 0.9482 - val_loss: 0.1227 - val_acc: 0.9644\nEpoch 4/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.1434 - acc: 0.9583 - val_loss: 0.1103 - val_acc: 0.9698\nEpoch 5/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.1235 - acc: 0.9642 - val_loss: 0.0952 - val_acc: 0.9732\nEpoch 6/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.1076 - acc: 0.9684 - val_loss: 0.0865 - val_acc: 0.9755\nEpoch 7/20\n33600/33600 [==============================] - 3s 84us/step - loss: 0.0994 - acc: 0.9699 - val_loss: 0.0978 - val_acc: 0.9731\nEpoch 8/20\n33600/33600 [==============================] - 2s 74us/step - loss: 0.0870 - acc: 0.9740 - val_loss: 0.0976 - val_acc: 0.9749\nEpoch 9/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0843 - acc: 0.9751 - val_loss: 0.0891 - val_acc: 0.9748\nEpoch 10/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0768 - acc: 0.9770 - val_loss: 0.0842 - val_acc: 0.9767\nEpoch 11/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.0683 - acc: 0.9797 - val_loss: 0.0868 - val_acc: 0.9750\nEpoch 12/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.0632 - acc: 0.9811 - val_loss: 0.0952 - val_acc: 0.9758\nEpoch 13/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0616 - acc: 0.9814 - val_loss: 0.0883 - val_acc: 0.9771\nEpoch 14/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0578 - acc: 0.9823 - val_loss: 0.0884 - val_acc: 0.9785\nEpoch 15/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0577 - acc: 0.9829 - val_loss: 0.0896 - val_acc: 0.9773\nEpoch 16/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0550 - acc: 0.9833 - val_loss: 0.0879 - val_acc: 0.9783\nEpoch 17/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0508 - acc: 0.9845 - val_loss: 0.0828 - val_acc: 0.9780\nEpoch 18/20\n33600/33600 [==============================] - 2s 72us/step - loss: 0.0514 - acc: 0.9841 - val_loss: 0.0898 - val_acc: 0.9775\nEpoch 19/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.0479 - acc: 0.9862 - val_loss: 0.0848 - val_acc: 0.9783\nEpoch 20/20\n33600/33600 [==============================] - 2s 71us/step - loss: 0.0429 - acc: 0.9872 - val_loss: 0.0863 - val_acc: 0.9783\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = pd.DataFrame(model4.predict(X_test, batch_size=200))\ntest_pred = pd.DataFrame(test_pred.idxmax(axis = 1))\ntest_pred.index.name = 'ImageId'\ntest_pred = test_pred.rename(columns = {0: 'Label'}).reset_index()\ntest_pred['ImageId'] = test_pred['ImageId'] + 1\n\ntest_pred.head()","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"   ImageId  Label\n0        1      2\n1        2      0\n2        3      9\n3        4      9\n4        5      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred.to_csv('mnist_submission.csv', index = False)","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Using this model, we are able to achieve a score of 0.976."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}